Working with Kinesis Firehose
------------------------

Create Delivery Stream
--name: TestDeliveryStream

Source can be (look for option 1 and option 2 mentioned below)
--direct put or other sources
--Kinesis data stream

====Transformation section ====
Tranformation (if required before storing it on s3)
Record Transformation: enabled
(using AWS lambda)
-create new lambda

-choose 'General Firehose Processing'
name: firehoseProcessing
role: create a role (full s3 access)

--Look into lamda function code and modify it
(for example adding a new line after each entry)
modification:
---
'use strict';
console.log('Loading function');

exports.handler = (event, context, callback) => {

	/*process the list of records and transform them */
	const output = event.records.map((record) => {
		
		let entry = (new Buffer(record.data, 'base64')).toString('utf8'); 
		let result = entry + "\n"					  
		const payload = (new Buffer(result , 'utf8')).toString('base64'); 
	
			return {
				recordId: record.recordId,
				result: 'Ok',
				data: payload,
			       };

		});
	console.log('Processing completed. Successful records ${output.length}.');
	callback(null, {records: output });
};
---
--change timeout to 1 min (requirement for kinesis)
--destination: s3 bucket of ur choice
--configure settings..
...
--choose appropriate role
===transformation section ends=====

=================
pushing data into delivery stream
we can take data from Kinesis Data stream or using commands from CLI
==========================
Option 1:
Kinesis Data Firehose (putting data using commands from CLI)
aws firehose put-record --delivery-stream-name xyz --record Data=100
Note**If required --cli-binary-format raw-in-base64-out

Option 2:
Kinesis Data Firehose (Put data in already created Data Stream and let Data firehose be used to transform or push data 
 to a destination)
--complete steps in "Kinesis_Data_Stream_n_AWS_CLI" upto describe only..
--After steps are completed , click your Data stream from console
--Under consumers 'Amazon Kinesis Data firehose' > process with delivery stream
--Delivery stream name: TestStrm
--Choose a source : Kinesis Data Stream (choose your already created data stream)
--Transformation : Disabled (or can be enabled and a lambda function can be used as described above)
--Format conversion : Disabled
--Destination : S3 > create new > strmdata
--Buffer conditions : 1 MiB, Buffer interval : 65 
--Compression : disabled
--Encryption : disabled
--Error Logging : enabled
--Permissions : choose existing IAM role : 
>>>>>create delivery stream..
(note** If your delivery stream creation fails due to role not having 'describeStream' permission,
edit your role and add 'inline policy' to access 'service: kinesis' and say 'all resources')

Once Created, it takes you to 'Kinesis Data Firehose delivery streams'
	
Name            Status   Creation time         Source       Data transformation Destination 

TestStrm	Active	2020-06-26T20:37+0200	StreamTest 	Disabled         Amazon S3 strmdata


Select TestStrm > Test with demo data (only works when source is not 'Data stream')

Lets put data into delivery stream from CLI by putting data into 'StreamTest' i.e. our 'Data Stream'
----
C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data 12 --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694298699603435439383765843970"
}


C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data 122 --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694299908529255054493976887298"
}


C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data 123 --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694301117455074669466748977154"
}


C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data 125 --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694304744232533513766589956098"
}


C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data 126 --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694310788861631587393499824130"
}


C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data hello --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694311997787451203190905634818"
}

----

Once puts are completed, wait as per your 'Buffer conditions' and then look into s3 bucket if data was delivered.

Amazon S3/strmdata/2020/06/26/18/TestStrm-1-2020-06-26-18-43-48-ada79c34-7644-4ab0-b172-99d50290c78d 

Select from > Show file preview.. 

Now Let's test Transformation:
Click and open your delivery stream i.e. 'TestStrm'
Edit > Source record transformation > enabled > follow as mentioned in "transformation section" above.
--Now test by pushing in some new data
---
C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data Newdata --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694316833490729755990726541314"
}


C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data Newdata11 --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694318042416549371100937584642"
}


C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data Newdata12 --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694319251342368986211148627970"
}


C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data Newdata13 --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694320460268188601390079148034"
}


C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data 13 --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694321669194008216431570714626"
}


C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data 14 --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694322878119827831610501234690"
}


C:\Users\Win10>aws kinesis put-record --stream-name StreamTest --partition-key 1 --data 15 --region us-east-1 --cli-binary-format raw-in-base64-out
{
    "ShardId": "shardId-000000000000",
    "SequenceNumber": "49608328425589687984827166694324087045647446720712278018"
}
----
Check data in S3



